{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class CFG:\n",
    "    # Data paths\n",
    "    data_path = \"/kaggle/input/llm-classification-finetuning/\" if IS_KAGGLE else \"llm-classification-finetuning/\"\n",
    "    \n",
    "    # Training config\n",
    "    max_length = 512\n",
    "    batch_size = 8\n",
    "    epochs = 3\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    seed = 42\n",
    "    num_labels = 3\n",
    "    \n",
    "    # Model paths - will be auto-detected on Kaggle\n",
    "    model_path = \"microsoft/deberta-v3-small\"\n",
    "    tokenizer_path = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "# Auto-detect model path on Kaggle\n",
    "def find_model_paths():\n",
    "    \"\"\"Search for DeBERTa model and tokenizer in Kaggle locations.\"\"\"\n",
    "    if not IS_KAGGLE:\n",
    "        return CFG.model_path, CFG.tokenizer_path\n",
    "    \n",
    "    model_path = None\n",
    "    tokenizer_path = None\n",
    "    \n",
    "    # Check /kaggle/working paths first (custom saved models)\n",
    "    working_model = \"/kaggle/working/model\"\n",
    "    working_tokenizer = \"/kaggle/working/tokenizer\"\n",
    "    \n",
    "    if os.path.exists(working_model) and os.path.exists(os.path.join(working_model, 'config.json')):\n",
    "        print(f\"✓ Found model at: {working_model}\")\n",
    "        model_path = working_model\n",
    "    \n",
    "    if os.path.exists(working_tokenizer) and os.path.exists(os.path.join(working_tokenizer, 'tokenizer_config.json')):\n",
    "        print(f\"✓ Found tokenizer at: {working_tokenizer}\")\n",
    "        tokenizer_path = working_tokenizer\n",
    "    \n",
    "    # If found in working directory, return\n",
    "    if model_path and tokenizer_path:\n",
    "        return model_path, tokenizer_path\n",
    "    \n",
    "    # Common paths where DeBERTa model might be located in /kaggle/input\n",
    "    possible_paths = [\n",
    "        \"/kaggle/input/deberta-v3-small\",\n",
    "        \"/kaggle/input/deberta-v3-small/deberta-v3-small\",\n",
    "        \"/kaggle/input/huggingface-deberta-v3-variants/deberta-v3-small\",\n",
    "        \"/kaggle/input/deberta-v-3-small/deberta-v3-small\",\n",
    "        \"/kaggle/input/deberta-v3-small/pytorch/small/1\",\n",
    "        \"/kaggle/input/deberta-v3-small/transformers/small/1\",\n",
    "    ]\n",
    "    \n",
    "    # Also search dynamically in /kaggle/input\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        for dataset in os.listdir('/kaggle/input'):\n",
    "            if 'deberta' in dataset.lower():\n",
    "                base_path = f'/kaggle/input/{dataset}'\n",
    "                possible_paths.insert(0, base_path)\n",
    "                if os.path.isdir(base_path):\n",
    "                    for subdir in os.listdir(base_path):\n",
    "                        possible_paths.insert(0, f'{base_path}/{subdir}')\n",
    "    \n",
    "    # Find path with config.json (indicates valid model directory)\n",
    "    for path in possible_paths:\n",
    "        if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "            print(f\"✓ Found model at: {path}\")\n",
    "            return path, path  # Same path for both model and tokenizer\n",
    "    \n",
    "    # If still not found, show what's available\n",
    "    print(\"Searching for model files...\")\n",
    "    print(\"Available in /kaggle/input:\")\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        for item in os.listdir('/kaggle/input'):\n",
    "            print(f\"  - {item}\")\n",
    "    print(\"\\nAvailable in /kaggle/working:\")\n",
    "    if os.path.exists('/kaggle/working'):\n",
    "        for item in os.listdir('/kaggle/working'):\n",
    "            print(f\"  - {item}\")\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        \"DeBERTa model not found! Please add a DeBERTa model dataset:\\n\"\n",
    "        \"1. Click '+ Add Data' in your Kaggle notebook\\n\"\n",
    "        \"2. Search for 'deberta-v3-small'\\n\"\n",
    "        \"3. Add one of the model datasets\"\n",
    "    )\n",
    "\n",
    "# Update model paths\n",
    "CFG.model_path, CFG.tokenizer_path = find_model_paths()\n",
    "\n",
    "# Set seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "print(f\"Model path: {CFG.model_path}\")\n",
    "print(f\"Tokenizer path: {CFG.tokenizer_path}\")\n",
    "print(f\"Data path: {CFG.data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fde1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(f\"{CFG.data_path}train.csv\")\n",
    "test_df = pd.read_csv(f\"{CFG.data_path}test.csv\")\n",
    "\n",
    "print(f\"Training data: {train_df.shape}\")\n",
    "print(f\"Test data: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def safe_literal_eval(x):\n",
    "    try:\n",
    "        return literal_eval(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove invalid Unicode characters\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='replace')\n",
    "    return text\n",
    "\n",
    "def create_input_text(row):\n",
    "    \"\"\"Create formatted input text from prompt and responses.\"\"\"\n",
    "    if isinstance(row['prompt_parsed'], list):\n",
    "        prompt_text = \" [TURN] \".join(str(p) for p in row['prompt_parsed'])\n",
    "    else:\n",
    "        prompt_text = str(row['prompt_parsed'])\n",
    "    \n",
    "    if isinstance(row['response_a_parsed'], list):\n",
    "        response_a_text = \" [TURN] \".join(str(r) for r in row['response_a_parsed'])\n",
    "    else:\n",
    "        response_a_text = str(row['response_a_parsed'])\n",
    "        \n",
    "    if isinstance(row['response_b_parsed'], list):\n",
    "        response_b_text = \" [TURN] \".join(str(r) for r in row['response_b_parsed'])\n",
    "    else:\n",
    "        response_b_text = str(row['response_b_parsed'])\n",
    "    \n",
    "    return f\"[PROMPT] {prompt_text} [RESPONSE_A] {response_a_text} [RESPONSE_B] {response_b_text}\"\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158670c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data\n",
    "train_df['prompt_parsed'] = train_df['prompt'].apply(safe_literal_eval)\n",
    "train_df['response_a_parsed'] = train_df['response_a'].apply(safe_literal_eval)\n",
    "train_df['response_b_parsed'] = train_df['response_b'].apply(safe_literal_eval)\n",
    "train_df['input_text'] = train_df.apply(create_input_text, axis=1)\n",
    "train_df['input_text'] = train_df['input_text'].apply(clean_text)\n",
    "\n",
    "# Create labels\n",
    "train_df['label'] = train_df.apply(\n",
    "    lambda x: 0 if x['winner_model_a'] == 1 else (1 if x['winner_model_b'] == 1 else 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Label distribution:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "print(\"\\n0=Model A wins, 1=Model B wins, 2=Tie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00933ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer (using separate tokenizer path)\n",
    "print(f\"Loading tokenizer from: {CFG.tokenizer_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['input_text'],\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_length,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc22ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val split\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df[['input_text', 'label']], \n",
    "    test_size=0.15, \n",
    "    stratify=train_df['label'],\n",
    "    random_state=CFG.seed\n",
    ")\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = HFDataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = HFDataset.from_pandas(val_data.reset_index(drop=True))\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['input_text'])\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['input_text'])\n",
    "\n",
    "# Rename label column\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "val_dataset = val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f\"Loading model from: {CFG.model_path}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.model_path,\n",
    "    num_labels=CFG.num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded! Device: {next(model.parameters()).device}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc32150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator and metrics\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    logloss = log_loss(labels, probs, labels=[0, 1, 2])\n",
    "    return {\"accuracy\": acc, \"log_loss\": logloss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74767149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for Kaggle GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Training on: {training_args.device}\")\n",
    "print(f\"FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_df['prompt_parsed'] = test_df['prompt'].apply(safe_literal_eval)\n",
    "test_df['response_a_parsed'] = test_df['response_a'].apply(safe_literal_eval)\n",
    "test_df['response_b_parsed'] = test_df['response_b'].apply(safe_literal_eval)\n",
    "test_df['input_text'] = test_df.apply(create_input_text, axis=1)\n",
    "test_df['input_text'] = test_df['input_text'].apply(clean_text)\n",
    "\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08036959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "def generate_predictions(model, tokenizer, texts, batch_size=16):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "# Generate test predictions\n",
    "test_predictions = generate_predictions(model, tokenizer, test_df['input_text'].tolist())\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a20d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'winner_model_a': test_predictions[:, 0],\n",
    "    'winner_model_b': test_predictions[:, 1],\n",
    "    'winner_tie': test_predictions[:, 2]\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission\n",
    "print(\"\\nSubmission Statistics:\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"\\nProbability ranges:\")\n",
    "print(f\"  winner_model_a: [{submission['winner_model_a'].min():.4f}, {submission['winner_model_a'].max():.4f}]\")\n",
    "print(f\"  winner_model_b: [{submission['winner_model_b'].min():.4f}, {submission['winner_model_b'].max():.4f}]\")\n",
    "print(f\"  winner_tie:     [{submission['winner_tie'].min():.4f}, {submission['winner_tie'].max():.4f}]\")\n",
    "print(f\"\\nRow sums (should be ~1.0): {submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1).mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
