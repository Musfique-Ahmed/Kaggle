{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7973dc3c",
   "metadata": {},
   "source": [
    "# LLM Classification Finetuning\n",
    "## Predict Human Preference using Chatbot Arena Conversations\n",
    "\n",
    "**Goal:** Predict which model's response a judge would prefer (model_a, model_b, or tie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e000dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c80fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "DATA_PATH = \"llm-classification-finetuning/\"\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}train.csv\")\n",
    "test_df = pd.read_csv(f\"{DATA_PATH}test.csv\")\n",
    "sample_sub = pd.read_csv(f\"{DATA_PATH}sample_submission.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61beede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore training data\n",
    "print(\"Training Data Columns:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210182f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "print(\"Training Data Info:\")\n",
    "train_df.info()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe06993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(\"Target Distribution:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Winner Model A: {train_df['winner_model_a'].sum()} ({train_df['winner_model_a'].mean()*100:.2f}%)\")\n",
    "print(f\"Winner Model B: {train_df['winner_model_b'].sum()} ({train_df['winner_model_b'].mean()*100:.2f}%)\")\n",
    "print(f\"Winner Tie:     {train_df['winner_tie'].sum()} ({train_df['winner_tie'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "targets = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "counts = [train_df[t].sum() for t in targets]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
    "bars = ax.bar(['Model A Wins', 'Model B Wins', 'Tie'], counts, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Winners in Training Data')\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "            f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the prompt and response columns (they are stored as string representations of lists)\n",
    "def safe_literal_eval(x):\n",
    "    try:\n",
    "        return literal_eval(x)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "# Parse columns\n",
    "train_df['prompt_parsed'] = train_df['prompt'].apply(safe_literal_eval)\n",
    "train_df['response_a_parsed'] = train_df['response_a'].apply(safe_literal_eval)\n",
    "train_df['response_b_parsed'] = train_df['response_b'].apply(safe_literal_eval)\n",
    "\n",
    "# Check sample parsed data\n",
    "print(\"Sample parsed prompt:\")\n",
    "print(train_df['prompt_parsed'].iloc[0])\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nSample parsed response_a:\")\n",
    "print(train_df['response_a_parsed'].iloc[0][:500] if isinstance(train_df['response_a_parsed'].iloc[0], list) else str(train_df['response_a_parsed'].iloc[0])[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97992939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Text lengths\n",
    "def get_text_length(x):\n",
    "    if isinstance(x, list):\n",
    "        return sum(len(str(item)) for item in x)\n",
    "    return len(str(x))\n",
    "\n",
    "def get_num_turns(x):\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    return 1\n",
    "\n",
    "train_df['prompt_length'] = train_df['prompt_parsed'].apply(get_text_length)\n",
    "train_df['response_a_length'] = train_df['response_a_parsed'].apply(get_text_length)\n",
    "train_df['response_b_length'] = train_df['response_b_parsed'].apply(get_text_length)\n",
    "train_df['num_turns'] = train_df['prompt_parsed'].apply(get_num_turns)\n",
    "train_df['length_diff'] = train_df['response_a_length'] - train_df['response_b_length']\n",
    "train_df['length_ratio'] = train_df['response_a_length'] / (train_df['response_b_length'] + 1)\n",
    "\n",
    "print(\"Feature Statistics:\")\n",
    "print(train_df[['prompt_length', 'response_a_length', 'response_b_length', 'num_turns', 'length_diff']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize response lengths by winner\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Create winner label\n",
    "train_df['winner'] = train_df.apply(\n",
    "    lambda x: 'Model A' if x['winner_model_a'] == 1 else ('Model B' if x['winner_model_b'] == 1 else 'Tie'), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Response A length by winner\n",
    "sns.boxplot(data=train_df, x='winner', y='response_a_length', ax=axes[0], palette=colors)\n",
    "axes[0].set_title('Response A Length by Winner')\n",
    "axes[0].set_ylim(0, train_df['response_a_length'].quantile(0.95))\n",
    "\n",
    "# Response B length by winner\n",
    "sns.boxplot(data=train_df, x='winner', y='response_b_length', ax=axes[1], palette=colors)\n",
    "axes[1].set_title('Response B Length by Winner')\n",
    "axes[1].set_ylim(0, train_df['response_b_length'].quantile(0.95))\n",
    "\n",
    "# Length difference by winner\n",
    "sns.boxplot(data=train_df, x='winner', y='length_diff', ax=axes[2], palette=colors)\n",
    "axes[2].set_title('Length Difference (A-B) by Winner')\n",
    "axes[2].set_ylim(train_df['length_diff'].quantile(0.05), train_df['length_diff'].quantile(0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce19c5c",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Model Input\n",
    "\n",
    "We'll create a combined text representation for each sample that includes:\n",
    "- The prompt\n",
    "- Response A\n",
    "- Response B\n",
    "\n",
    "This will be used as input to our transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc912d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_text(row):\n",
    "    \"\"\"\n",
    "    Create formatted input text from prompt and responses.\n",
    "    Format: [PROMPT] prompt_text [RESPONSE_A] response_a_text [RESPONSE_B] response_b_text\n",
    "    \"\"\"\n",
    "    # Handle list format (multi-turn conversations)\n",
    "    if isinstance(row['prompt_parsed'], list):\n",
    "        prompt_text = \" [TURN] \".join(str(p) for p in row['prompt_parsed'])\n",
    "    else:\n",
    "        prompt_text = str(row['prompt_parsed'])\n",
    "    \n",
    "    if isinstance(row['response_a_parsed'], list):\n",
    "        response_a_text = \" [TURN] \".join(str(r) for r in row['response_a_parsed'])\n",
    "    else:\n",
    "        response_a_text = str(row['response_a_parsed'])\n",
    "        \n",
    "    if isinstance(row['response_b_parsed'], list):\n",
    "        response_b_text = \" [TURN] \".join(str(r) for r in row['response_b_parsed'])\n",
    "    else:\n",
    "        response_b_text = str(row['response_b_parsed'])\n",
    "    \n",
    "    return f\"[PROMPT] {prompt_text} [RESPONSE_A] {response_a_text} [RESPONSE_B] {response_b_text}\"\n",
    "\n",
    "# Create combined text\n",
    "train_df['input_text'] = train_df.apply(create_input_text, axis=1)\n",
    "\n",
    "# Check sample\n",
    "print(\"Sample input text (truncated):\")\n",
    "print(train_df['input_text'].iloc[0][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target labels (0: model_a wins, 1: model_b wins, 2: tie)\n",
    "train_df['label'] = train_df.apply(\n",
    "    lambda x: 0 if x['winner_model_a'] == 1 else (1 if x['winner_model_b'] == 1 else 2),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "print(\"\\nLabel mapping: 0=Model A wins, 1=Model B wins, 2=Tie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f85a01",
   "metadata": {},
   "source": [
    "## Model Training with Transformers\n",
    "\n",
    "We'll use a pre-trained transformer model (DeBERTa) for this classification task. DeBERTa has shown strong performance on NLU tasks.\n",
    "\n",
    "**Approach:**\n",
    "1. Tokenize the combined text\n",
    "2. Fine-tune DeBERTa for 3-class classification\n",
    "3. Use cross-validation for robust evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers datasets accelerate -q\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class CFG:\n",
    "    model_name = \"microsoft/deberta-v3-small\"  # Can change to deberta-v3-base for better performance\n",
    "    max_length = 512  # Adjust based on GPU memory\n",
    "    batch_size = 8\n",
    "    epochs = 3\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    seed = 42\n",
    "    num_labels = 3\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "print(f\"Model: {CFG.model_name}\")\n",
    "print(f\"Max Length: {CFG.max_length}\")\n",
    "print(f\"Batch Size: {CFG.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['input_text'],\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_length,\n",
    "        padding=False  # Will use dynamic padding with DataCollator\n",
    "    )\n",
    "\n",
    "# Create train/validation split\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df[['input_text', 'label']], \n",
    "    test_size=0.15, \n",
    "    stratify=train_df['label'],\n",
    "    random_state=CFG.seed\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace datasets\n",
    "train_dataset = HFDataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = HFDataset.from_pandas(val_data.reset_index(drop=True))\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['input_text'])\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['input_text'])\n",
    "\n",
    "# Rename label column\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "val_dataset = val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "print(\"Dataset columns:\", train_dataset.column_names)\n",
    "print(\"Sample tokenized length:\", len(train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c53461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.model_name,\n",
    "    num_labels=CFG.num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    logloss = log_loss(labels, probs, labels=[0, 1, 2])\n",
    "    return {\"accuracy\": acc, \"log_loss\": logloss}\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31aedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size * 2,\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    seed=CFG.seed,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1784440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (uncomment to train)\n",
    "# trainer.train()\n",
    "\n",
    "# For quick testing, let's evaluate without training\n",
    "print(\"Note: Uncomment trainer.train() above to start training\")\n",
    "print(\"Training will take several hours depending on your GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5d5ac",
   "metadata": {},
   "source": [
    "## Inference and Submission\n",
    "\n",
    "Generate predictions on the test set and create submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca51da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_df['prompt_parsed'] = test_df['prompt'].apply(safe_literal_eval)\n",
    "test_df['response_a_parsed'] = test_df['response_a'].apply(safe_literal_eval)\n",
    "test_df['response_b_parsed'] = test_df['response_b'].apply(safe_literal_eval)\n",
    "test_df['input_text'] = test_df.apply(create_input_text, axis=1)\n",
    "\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(\"\\nSample test input (truncated):\")\n",
    "print(test_df['input_text'].iloc[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, tokenizer, texts, batch_size=16):\n",
    "    \"\"\"Generate predictions for test data.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "        \n",
    "        if (i + batch_size) % 100 == 0:\n",
    "            print(f\"Processed {min(i + batch_size, len(texts))}/{len(texts)} samples\")\n",
    "    \n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "# Generate predictions (after training)\n",
    "# test_predictions = generate_predictions(model, tokenizer, test_df['input_text'].tolist())\n",
    "\n",
    "print(\"Prediction function defined. Run after training completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "def create_submission(test_ids, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Create submission file.\n",
    "    predictions: numpy array of shape (n_samples, 3) with probabilities\n",
    "    \"\"\"\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'winner_model_a': predictions[:, 0],\n",
    "        'winner_model_b': predictions[:, 1],\n",
    "        'winner_tie': predictions[:, 2]\n",
    "    })\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"Submission saved to {output_path}\")\n",
    "    return submission\n",
    "\n",
    "# Example with dummy predictions (replace with actual predictions after training)\n",
    "dummy_preds = np.full((len(test_df), 3), 1/3)  # Equal probabilities as baseline\n",
    "submission = create_submission(test_df['id'].values, dummy_preds)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05275718",
   "metadata": {},
   "source": [
    "## Tips for Better Performance\n",
    "\n",
    "1. **Larger Models**: Use `deberta-v3-base` or `deberta-v3-large` for better performance\n",
    "2. **Longer Sequences**: Increase `max_length` to 1024 or 2048 if GPU memory allows\n",
    "3. **Cross-Validation**: Use 5-fold stratified CV and ensemble predictions\n",
    "4. **Data Augmentation**: Swap response_a and response_b with flipped labels\n",
    "5. **Gradient Accumulation**: Use to simulate larger batch sizes\n",
    "6. **Learning Rate Scheduling**: Try cosine annealing or linear decay\n",
    "7. **Model Ensembling**: Combine predictions from multiple models (DeBERTa, Longformer, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
